{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex_8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erodola/NumMeth-s2-2022/blob/main/esercizi/ex8/ex_8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline \n",
        "!wget https://raw.githubusercontent.com/riccardomarin/EG22_Tutorial_Spectral_Geometry/main/utils/utils_mesh.py\n",
        "!wget https://raw.githubusercontent.com/riccardomarin/EG22_Tutorial_Spectral_Geometry/main/utils/utils_spectral.py\n",
        "!pip install plyfile\n",
        "!wget https://raw.githubusercontent.com/riccardomarin/EG22_Tutorial_Spectral_Geometry/main/data/tr_reg_090.off\n",
        "!wget https://raw.githubusercontent.com/riccardomarin/EG22_Tutorial_Spectral_Geometry/main/data/tr_reg_043.off\n",
        "!wget https://github.com/riccardomarin/EG22_Tutorial_Spectral_Geometry/raw/main/data/minnesota_g.mat\n",
        "!wget https://github.com/riccardomarin/SpectralShapeAnalysis/raw/master/data/playstation.png\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from sklearn.cluster import KMeans\n",
        "import cv2 \n",
        "import scipy \n",
        "from utils_mesh import load_off\n",
        "from utils_mesh import plot_colormap, plot_RGBmap\n"
      ],
      "metadata": {
        "id": "PM9rnSy2uQlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa lezione vedremo una applicazione reale che mette insieme molte delle cose viste a lezione."
      ],
      "metadata": {
        "id": "AywwZi6RlmbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Laplacian"
      ],
      "metadata": {
        "id": "UFtWh0bKuNWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per prima cosa, vediamo di introdurre il Laplaciano: un operatore lineare che ha delle proprietà importanti in diversi contesti.\n",
        "\n",
        "Il Laplaciano è definito come:\n",
        "$\\Delta f = div(\\nabla f)) = \\sum_i \\frac{\\partial^2 f}{\\partial x_i^2}$; ovvero è la somma delle derivate parziali seconde di una funzione.\n",
        "\n",
        "Notate che nel caso in cui $f$ dipenda da solo una variabile, il laplaciano è esattamente la derivata seconda della funzione. $\\Delta f(x) = f''(x) = \\frac{d^2 f(x)}{dx^2}$\n",
        "\n",
        "Per la definizione di derivata discreta:\n",
        "\n",
        "$f'(x) = \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}$\n",
        "\n",
        "$f''(x) =  \\frac{f'(x_i) - f'(x_{i-1})}{x - x_{i-1}} = \\dots = - 2f(x_i) + f(x_{i-1}) + f(x_{i+1})$\n",
        "\n",
        "ESERCIZIO 1: date le $x$ e le $y$: \\\n",
        "- Calcolare la derivata prima discreta (usate la differenza tra $x_i$ e $x_{i+1}$)\n",
        "- Calcolare la derivata seconda discreta (usate la differenza tra $x_{i-1}$ e $x_i$)\n",
        "- Assumendo che le $x$ siano sempre uguali, come fareste una matrice $L$ che agisca come derivata seconda discreta, ovvero t.c. $Ly = f''$?\n"
      ],
      "metadata": {
        "id": "AvzL3WHDuVqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Le nostre x e le nostre y\n",
        "x = [1, 2,  3, 4,  5,  6, 7]\n",
        "y = [2, 5, 10, 3, -5, -3, 4]\n",
        "\n",
        "dy = []\n",
        "# calcolare la derivata prima:\n",
        "...\n",
        "\n",
        "print(f\"Derivata prima: {dy}\")\n",
        "\n",
        "ddy = []\n",
        "\n",
        "# calcolare la derivata seconda:\n",
        "...\n",
        "\n",
        "print(f\"Derivata seconda: {ddy}\")\n",
        "\n",
        "# costruire una matrice che agisca come derivata seconda.\n",
        "# Quindi idealmente: L @ y = [..., 2, -12. -1, 10, 5, ...]\n",
        "# I puntini sono perché la derivata seconda ai bordi non può essere esatta\n",
        "# (manca precedente a x_0 e antecedente a x_n)\n",
        "\n",
        "# Sfruttate la formula: ddy = - 2y_i + y_{i-1} + y_{i+1}\n",
        "\n",
        "# Inizializzo la matrice con gli zeri\n",
        "L = np.zeros((len(x),len(x)))\n",
        "\n",
        "\n",
        "# Cosa devo popolare della matrice?\n",
        "...\n",
        "\n",
        "print(L)\n",
        "print(f\"\\n\\nL @ y = {L @ y}\")"
      ],
      "metadata": {
        "id": "7b6IaHFdxNJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vediamo un modo più generale. Pensiamo al nostro asse x come un grafo corda:"
      ],
      "metadata": {
        "id": "L8WpNTsS1uHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1: Costruiamo una matrice di adiacenza per un grafo corda lungo 7\n",
        "n = 7\n",
        "\n",
        "A = np.zeros((n, n))\n",
        "a = np.array([i for i in np.arange(1,n)]) \n",
        "A[np.arange(a.size),a] = 1\n",
        "\n",
        "# Garantiamo la simmetria\n",
        "A = np.logical_or(A, A.T).astype(np.int32)\n",
        "\n",
        "# Otteniamo:\n",
        "# print(A) => np.array([  [0,1,0,0,0],\n",
        "#                         [1,0,1,0,0],\n",
        "#                         [0,1,0,1,0],\n",
        "#                         [0,0,1,0,1],\n",
        "#                         [0,0,0,1,0]])\n",
        "\n",
        "G = nx.from_numpy_matrix(A)\n",
        "\n",
        "# Posizione dei nodi per la visualizzazione\n",
        "pos = {i : np.asarray([i,0]) for i in np.arange(0,n)}\n",
        "\n",
        "# Visualizing the eig_n eigenfunction\n",
        "nx.draw(G, pos, node_size=40)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ZDBPQkzuPZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notate che il grafo corda ha fuori alla diagonale esattamente i valori della nostra matrice $L$. Rimane da popolare la diagonale, che non è altro il grado di ogni nodo moltiplicato per -1. L'unica differenza è ai bordi, che però non ci preoccupa (tanto lì possiamo solo approssimare)."
      ],
      "metadata": {
        "id": "jF1HSTEp3Fvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrice diagonale dei gradi\n",
        "D = np.diag(np.sum(A,1))\n",
        "\n",
        "# Graph Laplacian\n",
        "L_graph = A - D\n",
        "\n",
        "print(L_graph)\n",
        "print(f\"\\n\\nL @ y       = {(L @ y     ).astype(np.int32)}\")\n",
        "print(f\"\\n\\nL_graph @ y = {(L_graph @ y).astype(np.int32)}\")\n",
        "\n",
        "# Visualizziamo y\n",
        "nx.draw(G, pos, node_color=y , node_size=40, cmap=plt.cm.bwr, vmin=np.min(y), vmax=np.max(y))\n",
        "plt.show()\n",
        "\n",
        "# Visualizziamo L_graph @ y\n",
        "nx.draw(G, pos, node_color=L_graph @ y , node_size=40, cmap=plt.cm.bwr, vmin=np.min(y), vmax=np.max(y))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VJgw-tIq3jIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notate che abbiamo espresso la derivata seconda esclusivamente in funzione della matrice di connettività di grafo. $L = A - D$ è chiamato **Graph Laplacian**. Per una semplice questione di convenienza, da qui in avanti consideremo $L = - (A - D) = D - A$. Cambierà solo alcuni segni ma la logica rimarrà identica."
      ],
      "metadata": {
        "id": "xizqXN3J4Bcl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eigendecomposition del laplaciano"
      ],
      "metadata": {
        "id": "S1OnhU0Z78ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avete visto più volte che autovalori ed autovettori di matrici posso rappresentare delle quantità interpretabili, con delle proprietà molto interessanti. Anche il Laplaciano ha alcune di queste. Pensiamoci bene: cosa vuol dire per una funzione $f$ essere autovettore (anche detta *autofunzione*) del Laplaciano? Nel caso $1$D questo vuol dire semplicemente che derivando la funzione due volte otteniamo ancora una volta la stessa funzione, moltiplicata per uno scalare (l'autovalore)."
      ],
      "metadata": {
        "id": "dw65cpz78Bg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esercizio 2: \\\n",
        "- Trovare una funzione continua che può essere considerata autofunzione del Laplaciano\n",
        "- Calcolarla nel discreto (quindi ottente il vettore $y$) e provate ad applicarci il Laplaciano"
      ],
      "metadata": {
        "id": "Fhtb_yUD8tEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcolate le x. \n",
        "x = ...\n",
        "\n",
        "# Suggerimento: le x devono essere tra loro equidistanti. Per usare L_graph\n",
        "# definito sopra (matrice 7x7), il vettore delle y deve essere lungo 7.\n",
        "# La scelta della funzione che volete discretizzare vi può suggerire un\n",
        "# intervallo di campionamento.\n",
        "\n",
        "# Calcolate y, valutata nelle vostre x\n",
        "y = ...\n",
        "\n",
        "# Applicate L_graph\n",
        "ddy = ...\n",
        "\n",
        "# Visualizziamo il risultato\n",
        "print(y)\n",
        "print(ddy)\n",
        "\n",
        "# Differiscono? Certo! Ricordatevi che per un autovettore y, c'è anche un\n",
        "# autovettore \\lambda da considerare: L y = \\lambda y\n",
        "# Qual è questo \\lambda?\n",
        "\n",
        "print(...)"
      ],
      "metadata": {
        "id": "uvFKMwOQ9YYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grazie al fatto che abbiamo espresso L_graph attraverso una matrice, possiamo ottenere gli autovettori ed autovalori semplicemente con i metodi numerici visti nelle lezioni precedenti."
      ],
      "metadata": {
        "id": "XgbQhQXl_h64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evals, evecs = np.linalg.eigh(-L_graph)\n",
        "print(f\"autovalori: {evals}\")\n",
        "print(f\"autovettori:{evecs}\")\n",
        "\n",
        "# Visualizziamo la prima autofunzione\n",
        "nx.draw(G, pos, node_color=evecs[:,1] , node_size=40, cmap=plt.cm.bwr, vmin=np.min(evecs), vmax=np.max(evecs))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4YGlZL7p6qNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fatto interessante, è che il laplaciano è un operatore lineare, simmetrico, e a valori reali. Questo lo qualifica come operatore hermitiano. I suoi autovalori sono tutti non-negativi e l'insieme delle autofunzioni è una base ortonormale per lo spazio delle funzioni definite sul grafo. Vediamolo più concretamente."
      ],
      "metadata": {
        "id": "djF_onRvAhlX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esercizio 3: vi viene fornito un grafo a griglia, che rappresenta a tutti gli effetti un'immagine. \\\n",
        "- Definire il Laplaciano per questo grafo\n",
        "- Calcolare l'autodecomposizione\n",
        "- Utilizzare solo i primi $n$ autovettori per decomporre e ricostruire la funzione"
      ],
      "metadata": {
        "id": "CchTDvd0BJd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv2.imread('./playstation.png')  \n",
        "gray_img = np.mean(img,axis=2);\n",
        "\n",
        "G = nx.grid_2d_graph(gray_img.shape[0],gray_img.shape[1])\n",
        "\n",
        "pos = {(x,y):(y,-x) for x,y in G.nodes()}\n",
        "nx.draw(G, pos=pos, \n",
        "        node_color=gray_img, \n",
        "        node_size=20)"
      ],
      "metadata": {
        "id": "KJfwlaQqAhFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Otteniamo matrice di adiacenza e matrice dei gradi\n",
        "A = nx.adjacency_matrix(G)\n",
        "D = ...\n",
        "\n",
        "# Utilizziamo le atrici sparse per risparmiare memoria\n",
        "Ds = scipy.sparse.csr_matrix((D, (np.arange(0,A.shape[0]), np.arange(0,A.shape[0]))), shape=(A.shape[0], A.shape[1]),dtype=float)\n",
        "As = scipy.sparse.csr_matrix(A,dtype=float)\n",
        "\n",
        "# Calcolare il Laplaciano\n",
        "L = ...\n",
        "\n",
        "# Calcoliamo l'autodecomposizione\n",
        "evals, evecs = ...\n",
        "\n",
        "# Visualizziamo una autofunzione\n",
        "nx.draw(G, pos=pos, \n",
        "        node_color=evecs[:,2], \n",
        "        node_size=20)\n",
        "plt.show()\n",
        "\n",
        "# Usiamo le prime 400 autofunzioni\n",
        "n = 400\n",
        "\n",
        "# rendiamo l'immagine un vettore\n",
        "vec_img = ...\n",
        "\n",
        "# Proiettiamo il vettore nella base e ricostruiamolo\n",
        "low_pass_img = ...\n",
        "\n",
        "nx.draw(G, pos=pos, \n",
        "        node_color=low_pass_img, \n",
        "        node_size=20)"
      ],
      "metadata": {
        "id": "9uTXJFevDed0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functional Map e graph matching"
      ],
      "metadata": {
        "id": "1RsmnlRvIOGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Riassumiamo quanto visto fino ad adesso. Dato un grafo, siamo capaci di ricavarci un operatore lineare, i cui autovettori formano una base per lo spazio delle funzioni. Questi autovettori, ordinati per autovalori crescenti, ci permettono di avere un ordinamento consistente per \"frequenze\": i primi autovettori codificano informazione strutturale, mentre i successivi quella dei dettagli."
      ],
      "metadata": {
        "id": "7t3nYCtSIRr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Immaginiamo ora di avere due grafi, $G$ ed $F$. I due grafi avranno, in base alla loro diversa connettività, due matrici laplaciane diverse: $L_G$ e $L_F$, e così anche gli autovettori saranno diversi $\\Phi_G$ e $\\Phi_F$.\n",
        "\n",
        "A scopo di esempio immaginiamo che i due grafi che rappresentano degli oggetti 3D."
      ],
      "metadata": {
        "id": "r1KYQwDDJI2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tr_reg_090.off\") as f:\n",
        "  v_src, f_src = load_off(f)\n",
        "  f_src = np.asarray(f_src).astype('long')\n",
        "\n",
        "with open(\"tr_reg_043.off\") as f:\n",
        "  v_tar, f_tar = load_off(f)\n",
        "  f_tar = np.asarray(f_tar).astype('long')\n",
        "\n",
        "p = plot_colormap([v_src,v_tar], [f_src, f_tar],[np.ones(v_src.shape[0])]*2)\n",
        "p.show()"
      ],
      "metadata": {
        "id": "cMz3imyvEMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questi due oggetti hanno una corrispondenza naturale"
      ],
      "metadata": {
        "id": "2PXWTcGvKdUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "funz_ = (v_tar - np.min(v_tar,0))/np.tile((np.max(v_tar,0)-np.min(v_tar,0)),(np.size(v_tar,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_tar = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "\n",
        "p = plot_RGBmap([v_src, v_tar], [f_src,f_tar],[funz_tar,funz_tar])\n",
        "p.show()"
      ],
      "metadata": {
        "id": "a1DRBEroY8lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funzione per il calcolo del Wave Kernel Signature. Non ci interessa nei dettagli\n",
        "# è solo un modo per avere delle funzioni sulla superficie (simulando una diffusione)\n",
        "# di onde.\n",
        "\n",
        "def WKS(vertices, faces, evals, evecs, wks_size, variance):\n",
        "    # Number of vertices\n",
        "    n = vertices.shape[0]\n",
        "    WKS = np.zeros((n,wks_size))\n",
        "\n",
        "    # Just for numerical stability\n",
        "    evals[evals<1e-6] = 1e-6\n",
        "\n",
        "    # log(E)\n",
        "    log_E = np.log(evals).T\n",
        "\n",
        "    # Defining the energies step\n",
        "    e = np.linspace(log_E[1], np.max(log_E)/1.02, wks_size)\n",
        "\n",
        "    # Computing the sigma\n",
        "    sigma = (e[1]-e[0]) * variance\n",
        "    C = np.zeros((wks_size,1))\n",
        "\n",
        "    for i in np.arange(0,wks_size):\n",
        "        # Computing WKS\n",
        "        WKS[:,i] = np.sum(\n",
        "            (evecs)**2 * np.tile( np.exp((-(e[i] - log_E)**2) / (2*sigma**2)),(n,1)), axis=1)\n",
        "        \n",
        "        # Normalization\n",
        "        C[i] = np.sum(np.exp((-(e[i]-log_E)**2)/(2*sigma**2)))\n",
        "        \n",
        "    WKS = np.divide(WKS,np.tile(C,(1,n)).T)\n",
        "    return WKS"
      ],
      "metadata": {
        "id": "0Wic9lNIZJWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In questa cella calcoliamo il Laplaciano per superfici. Questo operatore è chiamato Laplace-Beltrami Operator. Non ci addentreremo nel dettaglio, ma la costruzione è uguale, solo che considera un grafo con dei pesi sui nodi (le aree) e sugli archi (in base agli angoli). Questo gli permette di tenere conto della geometria. A noi interesserà solo avere un operatore Laplaciano e la sua eigendecomposition."
      ],
      "metadata": {
        "id": "9Jg334ihiSH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from utils_spectral import LB_FEM_sparse, EigendecompositionSparse, LB_cotan, Eigendecomposition\n",
        "\n",
        "dtype = 'float32'\n",
        "k = 100\n",
        "\n",
        "# === COMPUING LBO EIGENFUNCTIONS ===\n",
        "v_src_t = torch.from_numpy(v_src.astype(dtype)).cuda()*1\n",
        "f_src_t = torch.from_numpy(np.asarray(f_src).astype('long')).cuda()\n",
        "\n",
        "L_sym_sp, A_sp_src, Ainv_sp = LB_FEM_sparse(v_src_t, f_src_t.long())\n",
        "evecs_src, evals_src = EigendecompositionSparse(L_sym_sp.values(),L_sym_sp.indices(), torch.tensor(k), torch.tensor(L_sym_sp.shape[-1]))\n",
        "evecs_src = evecs_src * Ainv_sp[:,None]\n",
        "\n",
        "v_tar_t = torch.from_numpy(v_tar.astype(dtype)).cuda()*1\n",
        "f_tar_t = torch.from_numpy(np.asarray(f_tar).astype('long')).cuda()\n",
        "\n",
        "L_sym_sp, A_sp_tar, Ainv_sp = LB_FEM_sparse(v_tar_t, f_tar_t.long())\n",
        "evecs_tar, evals_tar = EigendecompositionSparse(L_sym_sp.values(),L_sym_sp.indices(), torch.tensor(k), torch.tensor(L_sym_sp.shape[-1]))\n",
        "evecs_tar = evecs_tar * Ainv_sp[:,None]\n",
        "\n",
        "# === SAVING IN NUMPY ===\n",
        "evecs_tar = evecs_tar.detach().cpu().numpy()\n",
        "evals_tar = evals_tar.detach().cpu().numpy()\n",
        "area_tar = np.diag(A_sp_tar.detach().cpu().numpy())\n",
        "\n",
        "evecs_src = evecs_src.detach().cpu().numpy()\n",
        "evals_src = evals_src.detach().cpu().numpy()\n",
        "area_src = np.diag(A_sp_src.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "id": "ih5N5bIzZKEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facciamo come fatto prima:"
      ],
      "metadata": {
        "id": "82IWo3CEZhvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dichiariamo una funzione arbitraria\n",
        "f = np.zeros((v_tar.shape[0],1))\n",
        "f[v_tar[:,1] > 0.65] = 1\n",
        "\n",
        "# Proiettiamo e ricostruiamo la funzione. \n",
        "# Nota: sulle superfici il prodotto interno richiede di considerare\n",
        "# le aree (imita un'operazione di integrazione). Quindi semplicemente:\n",
        "# <F, G> = F.T @ Area @ G\n",
        "\n",
        "f_rec = evecs_tar @ evecs_tar.T @ area_tar @ f\n",
        "\n",
        "p = plot_colormap([ v_tar, v_tar], [ f_tar, f_tar],[ f, f_rec])\n",
        "p.show()"
      ],
      "metadata": {
        "id": "KRYQ0DlPZl6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il problema si pone quando dobbiamo trasferire questo vettore sull'altro modello. Di solito le due non hanno lo stesso grafo\\triangolazione. Dobbiamo quindi trovare un modo per trasferire la funzione."
      ],
      "metadata": {
        "id": "GFjBsIsma4HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cerchiamo quindi una $T_F:F_{M} \\rightarrow F_{N}$ che ci permetta di trasferire le funzioni di una shape sull'altra.\n",
        "\n",
        "Visto che abbiamo per $M$ e per $N$ delle basi di funzioni, un'idea è quella di trovare una matrice che ci permetta di trasferire i coefficienti. Ovvero, siano $A$ i coefficienti di una funzione $\\psi_A \\in F_{M}$, vorremmo idealmente trovare la matrice $C$ tale che:\n",
        "\n",
        "$B = CA$\n",
        "\n",
        "e $B$ siano i coefficienti adatti per la base funzionale di $N$ e siano associati alla funzione corrispondente $\\psi_B \\in F_{N}$."
      ],
      "metadata": {
        "id": "upa_xmMI0-sr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notate che se avessimo un buon numero di funzioni per cui conosciamo la trasformazione, potremmo risolvere per la matrice $C$.\n",
        "\n",
        "Esercizio 4: \n",
        "Dato un insieme di funzioni per ognuna delle due shape\n",
        "- Calcolare i coefficienti nelle basi (considereremo solo i primi 30 vettori)\n",
        "- Risolvere per la matrice C\n",
        "- Applicare la matrice C e trasferire la funzione che abbiamo definito nelle celle precedenti (quella che evidenziava la testa)\n",
        "\n",
        "Nota: se $ B = CA $ allora\n",
        "\n",
        "$C= \\dots$"
      ],
      "metadata": {
        "id": "2uGnNDfQ2HiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "\n",
        "# Le funzioni che useremo sono 100 funzioni indicatrici sparse \n",
        "# e alcune funzioni di diffusione generiche\n",
        "\n",
        "n_land =  100\n",
        "n_wks  =  20\n",
        "\n",
        "# Calcolo delle funzioni indicatrici\\Landmarks sparsi\n",
        "step = np.int(np.ceil(v_src.shape[0] / n_land))\n",
        "a = np.arange(0,v_src.shape[0],step)\n",
        "landmarks = np.zeros((v_src.shape[0], a.size))\n",
        "landmarks[a,np.arange(a.size)] = 1\n",
        "\n",
        "# Calcolo delle funzioni di diffusione\n",
        "d_src = WKS(v_src, f_src, evals_src, evecs_src, n_wks, 7)\n",
        "d_tar = WKS(v_tar, f_tar, evals_tar, evecs_tar, n_wks, 7)\n",
        "\n",
        "# Aggrego tutti i descrittori in un'unica matrice\n",
        "desc_src = np.hstack((landmarks,d_src))\n",
        "desc_tar = np.hstack((landmarks,d_tar))\n",
        "\n",
        "# Alla fine avremo 120 funzioni per ogni shape\n",
        "print(desc_src.shape)\n",
        "print(desc_tar.shape)"
      ],
      "metadata": {
        "id": "RMZOmUxba3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Intendiamo considerare solo le prime 30 basi\n",
        "n_evals = 30\n",
        "\n",
        "# Recupero i coefficienti per le tue shape\n",
        "# NOTA: in questo caso particolare, il prodotto interno tiene conto\n",
        "# delle aree. Significa che <F,G> = G.T @ A @ F\n",
        "# dove A è la matrice diagonale delle aree (nel nostro caso queste si chiamano\n",
        "# area_src e area_tar\n",
        "B = ...\n",
        "A = ...\n",
        "\n",
        "C = ...\n",
        "\n",
        "plt.imshow(C)\n",
        "\n",
        "# Ottengo i coefficienti per la funzione \"f\" definita su \"Tar\"\n",
        "coeff_f = ...\n",
        "\n",
        "# Li trasformo con la C, e ottengo i coefficenti per \"Src\"\n",
        "coeff_f_src = ...\n",
        "\n",
        "# Utilizzo le basi di \"Src\" per ricostruire la funzione\n",
        "funz_src = ...\n",
        "\n",
        "# Visualizzazione\n",
        "p = plot_colormap([ v_tar, v_src], [ f_tar, f_src],[ f_rec, funz_src])\n",
        "p.show()"
      ],
      "metadata": {
        "id": "WFNltGxn3ZlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pensate ora di definire una funzione indicatrice su ogni punto, e di trasferirla con la nostra $C$. Se la nostra $C$ funziona correttamente, otteniamo una corrispondenza! Ecco quindi un modo molto interessante di risolvere un matching punto a punto tra due oggetti 3D.\n",
        "\n",
        "In realtà si può dimostrare che trasferire le funzioni indicatrici è equivalente ad applicare la $C$ alle basi (cioè, invece di applicarla ai coefficienti si applica direttmaente alle autofunzioni della matrice del Laplaciano), e poi si fa nearest-neighbor tra le autofunzioni di una e dell'altra."
      ],
      "metadata": {
        "id": "_4WvZ7uk5eu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostriamo la C\n",
        "C_np = C\n",
        "plt.imshow(C_np)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Trasformo gli autovettori di \"Src\"\n",
        "evecs_src_map = evecs_src_map @ C_np.T\n",
        "\n",
        "# Faccio nearest-neighbor tra gli autovettori di \"Tar\" e gli autovettori trasferiti\n",
        "# di \"Src\"\n",
        "treesearch = sp.spatial.cKDTree(evecs_src_map)\n",
        "p2p = treesearch.query(evecs_tar[:,0:n_evals], k=1)[1]\n",
        "\n",
        "# Visualizziamo la corrispondenza\n",
        "funz_ = (v_src - np.min(v_src,0))/np.tile((np.max(v_src,0)-np.min(v_src,0)),(np.size(v_src,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_src = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "\n",
        "p = plot_RGBmap([v_src, v_tar], [f_src,f_tar],[funz_src,funz_src[p2p,:]])\n",
        "p.show()\n",
        "\n",
        "# Calcoliamo l'errore\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "metadata": {
        "id": "BXWAzI6YjIk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purtroppo però risolvere direttamente utilizzando una serie di funzioni in corrispondenza ci richiederebbe di avere parecchie funzioni buone. Spesso non è questo il caso, e si dispone solo di poca informazione. Quindi la $C$ può venire formulato come un problema di ottimizzazione.\n",
        "\n",
        "$\\arg\\min\\limits_C = \\|B - CA\\| + reg(C)$\n",
        "\n",
        "dove $reg(C)$ sono alcune regolarizzazioni."
      ],
      "metadata": {
        "id": "SlOelTyO7Xn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Qui riportiamo solo per praticità alcune variabili che ci saranno utili\n",
        "evecs_tar = evecs_tar\n",
        "evals_tar = evals_tar\n",
        "evecs_src = evecs_src\n",
        "evals_src = evals_src\n",
        "A_src = np.diag(A_sp_src.cpu().detach().numpy())\n",
        "A_tar = np.diag(A_sp_tar.cpu().detach().numpy())\n",
        "\n",
        "# Normalizziamo i descrittori\n",
        "no = np.sqrt(np.diag(np.matmul(A_src.__matmul__(desc_src).T, desc_src)))\n",
        "no_s = np.tile(no.T,(v_src.shape[0],1))\n",
        "no_t = np.tile(no.T,(v_tar.shape[0],1))\n",
        "fct_src = np.divide(desc_src,no_s)\n",
        "fct_tar = np.divide(desc_tar,no_t)\n",
        "\n",
        "# Calcoliamo i coefficienti dei decrittori\n",
        "Fct_src = np.matmul(A_src.T.__matmul__(evecs_src[:, 0:n_evals]).T, fct_src)\n",
        "Fct_tar = np.matmul(A_tar.T.__matmul__(evecs_tar[:, 0:n_evals]).T, fct_tar)\n",
        "\n",
        "# La relazione tra le autofunzioni costanti può essere ricavata in forma chiusa\n",
        "constFct = np.zeros((n_evals,1))\n",
        "constFct[0, 0] = np.sign(evecs_src[0, 0] * evecs_tar[0, 0]) * np.sqrt(np.sum(A_tar)/np.sum(A_src))\n",
        "\n",
        "# Impostiamo i pesi\n",
        "w1 = 1e-1 # Descriptors preservation\n",
        "w2 = 1e-8 # Commutativity with Laplacian\n",
        "\n",
        "# Define PyTorch objects\n",
        "fs = torch.Tensor(Fct_src)\n",
        "ft = torch.Tensor(Fct_tar)\n",
        "evals = torch.diag(torch.Tensor(np.reshape(np.float32(evals_src[0:n_evals]), (n_evals,))))\n",
        "evalt = torch.diag(torch.Tensor(np.reshape(np.float32(evals_tar[0:n_evals]), (n_evals,))))"
      ],
      "metadata": {
        "id": "r96JQM8yiZHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esercizio 5: \n",
        "- Effettuare la discesa del gradiente per ottimizzare C\n",
        "\n",
        "Nella soluzione vedremo una libreria per effettuare la differenziazione automatica"
      ],
      "metadata": {
        "id": "Exy9pJn48XDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import progressbar\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Inizializziamo la C\n",
        "C_ini = np.zeros((n_evals,n_evals))\n",
        "C_ini[0,0]=constFct[0,0]\n",
        "C = Variable(torch.Tensor(C_ini), requires_grad=True)\n",
        "\n",
        "# Applicate gradient descent\n",
        "\n",
        "for it in progressbar.progressbar(range(1500)):   \n",
        "\n",
        "    ...\n",
        "    \n",
        "    # La loss è data da due termini. Il secondo è una regolarizzazione\n",
        "    # e verifica che la C commuti con l'operatore del Laplaciano (anche qui, \n",
        "    # risparmiamo i dettagli)\n",
        "    loss1 = w1 * torch.sum(((torch.matmul(C, fs) - ft) ** 2)) / 2 # Descriptor preservation\n",
        "    loss2 = w2 * torch.sum((torch.matmul(C, evals) - torch.matmul(evalt,C))**2) # Commute with Laplacian\n",
        "    loss = torch.sum(loss1  + loss2)\n",
        "\n",
        "    ..."
      ],
      "metadata": {
        "id": "cA5iHScHin1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizziamo la matrice C\n",
        "C_np = C.detach().numpy()\n",
        "plt.imshow(C_np)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Corrispondenza punto a punto\n",
        "treesearch = sp.spatial.cKDTree(np.matmul(evecs_src[:,0:n_evals], C_np.T))\n",
        "p2p = treesearch.query(evecs_tar[:,0:n_evals], k=1)[1]\n",
        "\n",
        "# Visualizziamo la corrispondenza\n",
        "funz_ = (v_src - np.min(v_src,0))/np.tile((np.max(v_src,0)-np.min(v_src,0)),(np.size(v_src,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_src = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "\n",
        "p = plot_RGBmap([v_src, v_tar], [f_src,f_tar],[funz_src,funz_src[p2p,:]])\n",
        "p.show()\n",
        "\n",
        "# Calcoliamo l'errore\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p,:]))\n",
        "print(err)"
      ],
      "metadata": {
        "id": "1VLRF65UjBIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ICP Refinement\n",
        "The correspondence can be post-processed by ICP registration in the eigenfunction space!\n",
        "\n",
        "Esercizio 6:\n",
        "- Applicate ICP (Visto all'esercitazione 6) tra le due basi, raffinando la $C$ a ogni iterazione"
      ],
      "metadata": {
        "id": "XK4gvYSq0-WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('ICP refine...')\n",
        "\n",
        "# Iniziamo con la C che abbiamo già\n",
        "C_ICP = C_np\n",
        "\n",
        "# Iteriamo per 5 volte\n",
        "for k in np.arange(0,5):\n",
        "    # Trova i matching \n",
        "    ...\n",
        "\n",
        "    # Calcoliamo la matrice \n",
        "    W = ...\n",
        "    \n",
        "    d = np.linalg.svd(W)\n",
        "    C_ICP = ...\n",
        "\n",
        "# C Visualization\n",
        "plt.imshow(C_ICP)\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "# Correspondence visualization \n",
        "treesearch = scipy.spatial.cKDTree(np.matmul(evecs_src[:,0:n_evals],C_ICP.T))\n",
        "p2p_icp = treesearch.query(evecs_tar[:,0:n_evals], k=1)[1]\n",
        "\n",
        "# Correspondence visualization\n",
        "funz_ = (v_src - np.min(v_src,0))/np.tile((np.max(v_src,0)-np.min(v_src,0)),(np.size(v_src,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_src = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "\n",
        "p = plot_RGBmap([v_src, v_tar], [f_src,f_tar],[funz_src,funz_src[p2p_icp,:]])\n",
        "p.show()\n",
        "\n",
        "# Computing (euclidean) error evaluation\n",
        "err = np.sum(np.square(v_tar - v_tar[p2p_icp,:]))\n",
        "print(err)"
      ],
      "metadata": {
        "id": "kUdyX_Ppi_5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Possiamo anche visualizzare funzioni più ad alte frequenze, per meglio osservare la qualità del matching"
      ],
      "metadata": {
        "id": "8hKYwFB12R4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correspondence visualization\n",
        "funz_ = (v_src - np.min(v_src,0))/np.tile((np.max(v_src,0)-np.min(v_src,0)),(np.size(v_src,0),1));\n",
        "colors = np.cos(funz_);\n",
        "funz_src = (colors-np.min(colors))/(np.max(colors) - np.min(colors));\n",
        "\n",
        "# Higher frequencies function\n",
        "funz_src  = np.cos(funz_src * 10)\n",
        "\n",
        "p = plot_RGBmap([v_src, v_tar, v_tar, v_tar], [f_src,f_tar, f_tar,f_tar,f_tar],\n",
        "                [funz_src, funz_src, funz_src[p2p,:],funz_src[p2p_icp,:]])\n",
        "p.show()"
      ],
      "metadata": {
        "id": "kuh063WsjXiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da sinistra: \n",
        "\n",
        "Source - Target (GT) - Target (FMAP) - Target (FMAP + ICP)"
      ],
      "metadata": {
        "id": "2fsWEGeY4Xi7"
      }
    }
  ]
}